{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b52d5612",
      "metadata": {
        "id": "b52d5612"
      },
      "source": [
        "# SVM & Naïve Bayes — Question-by-Question (Detailed Answers + Code)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cba39c4",
      "metadata": {
        "id": "1cba39c4"
      },
      "source": [
        "## Part A — Theory (Question-by-question answers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bda4711",
      "metadata": {
        "id": "9bda4711"
      },
      "source": [
        "### Q1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression. For classification, SVM finds the hyperplane (in input feature space) that maximizes the margin between classes — the margin being the distance between the hyperplane and the nearest training points from each class. SVM is effective in high-dimensional spaces and can use kernel functions to handle non-linearly separable data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09098853",
      "metadata": {
        "id": "09098853"
      },
      "source": [
        "### Q2. What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Hard Margin SVM** requires the training data to be linearly separable and finds a hyperplane that makes zero classification errors. It maximizes the margin subject to all points being correctly classified. This is sensitive to noise/outliers.\n",
        "\n",
        "**Soft Margin SVM** introduces slack variables (ξ_i) allowing some misclassifications. It trades off margin width and misclassification using a penalty parameter C. Soft margin is robust to non-separable data and noise.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e274f8a",
      "metadata": {
        "id": "1e274f8a"
      },
      "source": [
        "### Q3. What is the mathematical intuition behind SVM?\n",
        "\n",
        "**Answer (mathematical intuition):**\n",
        "\n",
        "SVM solves the optimization problem:\n",
        "\n",
        "Minimize (1/2) ||w||^2 subject to y_i (w·x_i + b) ≥ 1 for all i (separable case).\n",
        "\n",
        "Maximizing the margin is equivalent to minimizing the norm of the weight vector w while enforcing correct classification constraints. For non-separable data, slack variables ξ_i are added: y_i (w·x_i + b) ≥ 1 − ξ_i, with ξ_i ≥ 0, and the objective becomes\n",
        "\n",
        "Minimize (1/2)||w||^2 + C Σ ξ_i\n",
        "\n",
        "where C controls the penalty for misclassification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbcd273d",
      "metadata": {
        "id": "cbcd273d"
      },
      "source": [
        "### Q4. What is the role of Lagrange Multipliers in SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Lagrange multipliers convert the constrained primal optimization problem into an unconstrained dual problem. The dual formulation depends only on dot products between training vectors (x_i·x_j). Lagrange multipliers α_i are non-zero only for support vectors; solving the dual is computationally advantageous and enables kernelization (replacing dot products with kernels).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d54a0ca",
      "metadata": {
        "id": "9d54a0ca"
      },
      "source": [
        "### Q5. What are Support Vectors in SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Support vectors are the training samples that lie closest to the decision boundary (i.e., on the margin or inside it). In the dual problem they correspond to non-zero Lagrange multipliers α_i. They uniquely define the separating hyperplane — removing non-support vectors does not change the solution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c863375",
      "metadata": {
        "id": "5c863375"
      },
      "source": [
        "### Q6. What is a Support Vector Classifier (SVC)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "A Support Vector Classifier (SVC) typically refers to the SVM algorithm used for classification tasks. It separates data into discrete categories by finding the optimal hyperplane with maximum margin. In scikit-learn, `sklearn.svm.SVC` implements the classifier with kernels.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20bc6dfe",
      "metadata": {
        "id": "20bc6dfe"
      },
      "source": [
        "### Q7. What is a Support Vector Regressor (SVR)?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Support Vector Regressor (SVR) applies the SVM idea to regression. SVR tries to fit a function that deviates from target values by at most ε for all training data while being as flat as possible. It uses an ε-insensitive loss and can also be kernelized. In scikit-learn, it's `sklearn.svm.SVR`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddb1932",
      "metadata": {
        "id": "5ddb1932"
      },
      "source": [
        "### Q8. What is the Kernel Trick in SVM?\n",
        "\n",
        "**Answer (Kernel Trick):**\n",
        "\n",
        "The kernel trick replaces the dot product x·z in the SVM dual formulation with a kernel function K(x, z) that corresponds to an inner product in some (often higher-dimensional) feature space φ(x)·φ(z). This allows the algorithm to learn non-linear decision boundaries without explicitly computing φ(x), avoiding the computational cost of transforming to high-dimensional spaces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c690df9",
      "metadata": {
        "id": "8c690df9"
      },
      "source": [
        "### Q9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "\n",
        "**Answer (Compare kernels):**\n",
        "\n",
        "- **Linear kernel:** K(x,y) = x·y. Use when data are linearly separable or when number of features is large relative to samples (linear decision boundary). Fast and fewer hyperparameters.\n",
        "\n",
        "- **Polynomial kernel:** K(x,y) = (γ x·y + r)^d. Allows polynomial decision boundaries. Degree d controls complexity; can capture interactions up to degree d. More hyperparameters (degree, γ, r).\n",
        "\n",
        "- **RBF (Gaussian) kernel:** K(x,y) = exp(−γ||x − y||^2). Highly flexible, maps to infinite-dimensional space. γ controls kernel width. Good default when no prior on feature interactions.\n",
        "\n",
        "Trade-offs: linear is simple/fast; polynomial can capture global non-linearities of certain degrees; RBF is flexible but risks overfitting if γ is large.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a9565a8",
      "metadata": {
        "id": "4a9565a8"
      },
      "source": [
        "### Q10. What is the effect of the C parameter in SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "`C` is the regularization parameter controlling the penalty for misclassification. High `C` values aim to minimize misclassification (small margin, possible overfitting). Low `C` values increase regularization, allowing more misclassified points but increasing margin (simpler model).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "703045fa",
      "metadata": {
        "id": "703045fa"
      },
      "source": [
        "### Q11. What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "In RBF kernel SVM, γ (gamma) defines how much influence a single training example has. Low γ → far reach, smoother decision boundary. High γ → narrow reach, complex/wiggly boundary that may overfit. It effectively sets the kernel bandwidth.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12d2ee80",
      "metadata": {
        "id": "12d2ee80"
      },
      "source": [
        "### Q12. What is the Naïve Bayes classifier, and why is it called 'Naïve'?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Naïve Bayes is a probabilistic classifier that applies Bayes' theorem with the simplifying assumption that features are conditionally independent given the class label. It's called 'naïve' because this independence assumption is often unrealistic in practice.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c690d970",
      "metadata": {
        "id": "c690d970"
      },
      "source": [
        "### Q13. What is Bayes’ Theorem?\n",
        "\n",
        "**Answer (Bayes' Theorem):**\n",
        "\n",
        "Bayes' theorem states:\n",
        "\n",
        "P(C|X) = P(X|C) P(C) / P(X)\n",
        "\n",
        "where P(C|X) is the posterior probability of class C given features X, P(X|C) is the likelihood, P(C) is the prior, and P(X) is evidence (normalizing constant).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "499ce63e",
      "metadata": {
        "id": "499ce63e"
      },
      "source": [
        "### Q14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n",
        "\n",
        "**Answer (Variants):**\n",
        "\n",
        "- **Gaussian Naive Bayes:** Assumes continuous features follow a Gaussian distribution within each class. Likelihood P(x_j|C) modeled as Normal(μ_{Cj}, σ_{Cj}^2).\n",
        "\n",
        "- **Multinomial Naive Bayes:** Designed for count features (e.g., term frequencies). Likelihood is multinomial over term counts; common for document classification.\n",
        "\n",
        "- **Bernoulli Naive Bayes:** Models binary/boolean features (presence/absence). Suitable when features are binary indicators of token presence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "792f17cc",
      "metadata": {
        "id": "792f17cc"
      },
      "source": [
        "### Q15. When should you use Gaussian Naïve Bayes over other variants?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Use Gaussian NB when your features are continuous and reasonably approximated by Gaussian distributions (e.g., measurements, lab values). If features are counts or binary indicators, prefer Multinomial or Bernoulli respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd494fb2",
      "metadata": {
        "id": "bd494fb2"
      },
      "source": [
        "### Q16. What are the key assumptions made by Naïve Bayes?\n",
        "\n",
        "**Answer (Key assumptions):**\n",
        "\n",
        "- Conditional independence: features are independent given the class label.\n",
        "- Feature distributions fit the assumed family (e.g., Gaussian for Gaussian NB).\n",
        "- Class priors are known or estimated from training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c9a278",
      "metadata": {
        "id": "e9c9a278"
      },
      "source": [
        "### Q17. What are the advantages and disadvantages of Naïve Bayes?\n",
        "\n",
        "**Answer (Advantages & Disadvantages):**\n",
        "\n",
        "Advantages:\n",
        "- Fast and memory-efficient.\n",
        "- Performs well on high-dimensional data (like text).\n",
        "- Requires relatively small training data.\n",
        "- Simple to implement and interpret.\n",
        "\n",
        "Disadvantages:\n",
        "- Strong independence assumption often violated.\n",
        "- Can give poor probability estimates.\n",
        "- Less flexible than discriminative methods when feature interactions matter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb0be8c7",
      "metadata": {
        "id": "eb0be8c7"
      },
      "source": [
        "### Q18. Why is Naïve Bayes a good choice for text classification?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Naïve Bayes suits text classification because text representations (bag-of-words) are high-dimensional and sparse; the independence assumption becomes less harmful. Multinomial NB models word counts directly and trains extremely fast, making it a strong baseline for NLP tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3200032",
      "metadata": {
        "id": "c3200032"
      },
      "source": [
        "### Q19. Compare SVM and Naïve Bayes for classification tasks.\n",
        "\n",
        "**Answer (Compare SVM vs Naïve Bayes):**\n",
        "\n",
        "- **SVM:** Discriminative; often higher classification accuracy on many tasks; can use kernels for complex boundaries; slower on very large feature sets if kernelized. Needs careful tuning (C, gamma).\n",
        "- **Naïve Bayes:** Generative; extremely fast and robust with small data; works well for text; less tuned but may be outperformed by SVMs in many cases. NB gives simple probability outputs; SVM gives margins and can provide probability estimates via calibration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac8445a",
      "metadata": {
        "id": "6ac8445a"
      },
      "source": [
        "### Q20. How does Laplace Smoothing help in Naïve Bayes?\n",
        "\n",
        "**Answer (Laplace smoothing):**\n",
        "\n",
        "Laplace smoothing (additive smoothing) adds a small constant (usually 1) to observed feature counts to avoid zero probabilities for unseen features in test data. For Multinomial NB, it prevents P(word|class)=0 when a word not present in training for that class appears in test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "748e0bf4",
      "metadata": {
        "id": "748e0bf4"
      },
      "source": [
        "## Part B — Practical (Code question-by-question)\n",
        "\n",
        "Each practical question is followed by a runnable code cell (detailed). Run sequentially in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3db39437",
      "metadata": {
        "id": "3db39437"
      },
      "outputs": [],
      "source": [
        "# Shared imports for practical tasks\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, mean_squared_error, mean_absolute_error, roc_auc_score, log_loss, precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score\n",
        "print('Practical imports ready')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc233f4d",
      "metadata": {
        "id": "bc233f4d"
      },
      "source": [
        "### P1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df8a620",
      "metadata": {
        "id": "8df8a620"
      },
      "outputs": [],
      "source": [
        "# P1: SVM Classifier on Iris dataset\n",
        "from sklearn.svm import SVC\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "# Use all three classes\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', probability=True, random_state=0))])\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "print('Accuracy (Iris, 3-class):', accuracy_score(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d47284d",
      "metadata": {
        "id": "3d47284d"
      },
      "source": [
        "### P2. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d3b61ab",
      "metadata": {
        "id": "9d3b61ab"
      },
      "outputs": [],
      "source": [
        "# P2: Linear vs RBF on Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, test_size=0.2, random_state=1, stratify=yw)\n",
        "pipe_lin = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='linear', random_state=1))])\n",
        "pipe_rbf = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=1))])\n",
        "pipe_lin.fit(Xw_train, yw_train)\n",
        "pipe_rbf.fit(Xw_train, yw_train)\n",
        "print('Linear SVM accuracy:', accuracy_score(yw_test, pipe_lin.predict(Xw_test)))\n",
        "print('RBF SVM accuracy:   ', accuracy_score(yw_test, pipe_rbf.predict(Xw_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "397fdf2c",
      "metadata": {
        "id": "397fdf2c"
      },
      "source": [
        "### P3. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624f0402",
      "metadata": {
        "id": "624f0402"
      },
      "outputs": [],
      "source": [
        "# P3: SVR on California Housing\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "Xh, yh = housing.data, housing.target\n",
        "Xh_train, Xh_test, yh_train, yh_test = train_test_split(Xh, yh, test_size=0.2, random_state=0)\n",
        "pipe_svr = Pipeline([('scaler', StandardScaler()), ('svr', SVR(kernel='rbf'))])\n",
        "pipe_svr.fit(Xh_train, yh_train)\n",
        "y_pred = pipe_svr.predict(Xh_test)\n",
        "print('SVR MSE:', mean_squared_error(yh_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd7b4e4",
      "metadata": {
        "id": "fdd7b4e4"
      },
      "source": [
        "### P4. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbba5ea",
      "metadata": {
        "id": "4fbba5ea"
      },
      "outputs": [],
      "source": [
        "# P4: Polynomial kernel SVM decision boundary (2D toy data)\n",
        "from sklearn.datasets import make_circles\n",
        "Xc, yc = make_circles(noise=0.1, factor=0.2, random_state=1)\n",
        "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.25, random_state=1)\n",
        "clf_poly = SVC(kernel='poly', degree=3, coef0=1, C=1.0, probability=True, random_state=0)\n",
        "clf_poly.fit(Xc_train, yc_train)\n",
        "# visualize\n",
        "xx, yy = np.meshgrid(np.linspace(Xc[:,0].min()-0.5, Xc[:,0].max()+0.5, 300), np.linspace(Xc[:,1].min()-0.5, Xc[:,1].max()+0.5, 300))\n",
        "Z = clf_poly.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "plt.scatter(Xc[:,0], Xc[:,1], c=yc, s=30, edgecolor='k')\n",
        "plt.title('Polynomial Kernel SVM Decision Boundary')\n",
        "plt.show()\n",
        "print('Train acc:', clf_poly.score(Xc_train, yc_train), 'Test acc:', clf_poly.score(Xc_test, yc_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2b49363",
      "metadata": {
        "id": "b2b49363"
      },
      "source": [
        "### P5. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2009ed16",
      "metadata": {
        "id": "2009ed16"
      },
      "outputs": [],
      "source": [
        "# P5: Gaussian Naive Bayes on Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xct_train, Xct_test, yct_train, yct_test = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(Xct_train, yct_train)\n",
        "yp = gnb.predict(Xct_test)\n",
        "print('Gaussian NB accuracy (breast cancer):', accuracy_score(yct_test, yp))\n",
        "print(classification_report(yct_test, yp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ce96cb9",
      "metadata": {
        "id": "2ce96cb9"
      },
      "source": [
        "### P6. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "daf9d22e",
      "metadata": {
        "id": "daf9d22e"
      },
      "outputs": [],
      "source": [
        "# P6: Multinomial NB on 20 Newsgroups (subset for speed)\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "categories = ['alt.atheism', 'sci.space', 'comp.graphics', 'rec.sport.baseball']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers','footers','quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers','footers','quotes'))\n",
        "pipe_text = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())\n",
        "pipe_text.fit(newsgroups_train.data, newsgroups_train.target)\n",
        "pred = pipe_text.predict(newsgroups_test.data)\n",
        "print('Multinomial NB accuracy (20 Newsgroups subset):', accuracy_score(newsgroups_test.target, pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a18f46",
      "metadata": {
        "id": "73a18f46"
      },
      "source": [
        "### P7. Train an SVM Classifier with different C values and compare the decision boundaries visually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be61304c",
      "metadata": {
        "id": "be61304c"
      },
      "outputs": [],
      "source": [
        "# P7: SVM with different C values and visualize decision boundary (2D toy)\n",
        "from sklearn.datasets import make_classification\n",
        "X2, y2 = make_classification(n_samples=200, n_features=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
        "Xs_train, Xs_test, ys_train, ys_test = train_test_split(X2, y2, test_size=0.3, random_state=0)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15,4))\n",
        "Cs = [0.01, 1, 100]\n",
        "for ax, C in zip(axes, Cs):\n",
        "    clf = SVC(kernel='rbf', C=C, gamma='scale')\n",
        "    clf.fit(Xs_train, ys_train)\n",
        "    xx, yy = np.meshgrid(np.linspace(X2[:,0].min()-1, X2[:,0].max()+1, 300), np.linspace(X2[:,1].min()-1, X2[:,1].max()+1, 300))\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "    ax.contourf(xx, yy, Z, alpha=0.3)\n",
        "    ax.scatter(X2[:,0], X2[:,1], c=y2, edgecolor='k')\n",
        "    ax.set_title(f'C={C} | Train acc={clf.score(Xs_train, ys_train):.2f} | Test acc={clf.score(Xs_test, ys_test):.2f}')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3937d889",
      "metadata": {
        "id": "3937d889"
      },
      "source": [
        "### P8. Train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a01daec",
      "metadata": {
        "id": "5a01daec"
      },
      "outputs": [],
      "source": [
        "# P8: BernoulliNB on synthetic binary dataset\n",
        "from sklearn.datasets import make_classification\n",
        "Xb, yb = make_classification(n_samples=300, n_features=10, n_informative=5, n_redundant=0, random_state=0)\n",
        "# binarize features to simulate presence/absence\n",
        "Xb_bin = (Xb > np.median(Xb, axis=0)).astype(int)\n",
        "Xb_train, Xb_test, yb_train, yb_test = train_test_split(Xb_bin, yb, test_size=0.2, random_state=0)\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(Xb_train, yb_train)\n",
        "print('BernoulliNB accuracy (binary features):', accuracy_score(yb_test, bnb.predict(Xb_test)))\n",
        "print(classification_report(yb_test, bnb.predict(Xb_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78faac81",
      "metadata": {
        "id": "78faac81"
      },
      "source": [
        "### P9. Apply feature scaling before training an SVM model and compare results with unscaled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6731495",
      "metadata": {
        "id": "a6731495"
      },
      "outputs": [],
      "source": [
        "# P9: Effect of feature scaling on SVM\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, test_size=0.2, random_state=1, stratify=yw)\n",
        "# Without scaling\n",
        "clf_noscale = SVC(kernel='rbf', random_state=0)\n",
        "clf_noscale.fit(Xw_train, yw_train)\n",
        "acc_noscale = accuracy_score(yw_test, clf_noscale.predict(Xw_test))\n",
        "# With scaling\n",
        "clf_scale = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=0))])\n",
        "clf_scale.fit(Xw_train, yw_train)\n",
        "acc_scale = accuracy_score(yw_test, clf_scale.predict(Xw_test))\n",
        "print('Wine accuracy without scaling:', acc_noscale)\n",
        "print('Wine accuracy with scaling:   ', acc_scale)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21f4eafd",
      "metadata": {
        "id": "21f4eafd"
      },
      "source": [
        "### P10. Train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1383e033",
      "metadata": {
        "id": "1383e033"
      },
      "outputs": [],
      "source": [
        "# P10: GaussianNB predictions before and after Laplace smoothing (demonstration with MultinomialNB for counts)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "docs = ['spam spam offer', 'spam cheap offer', 'hello how are you', 'hello friend']\n",
        "y_docs = [1,1,0,0]\n",
        "cv = CountVectorizer()\n",
        "Xdocs = cv.fit_transform(docs)\n",
        "# alpha small vs alpha=1\n",
        "mnb_no = MultinomialNB(alpha=1e-9)\n",
        "mnb_yes = MultinomialNB(alpha=1.0)\n",
        "mnb_no.fit(Xdocs, y_docs)\n",
        "mnb_yes.fit(Xdocs, y_docs)\n",
        "print('Feature names:', cv.get_feature_names_out())\n",
        "print('Probs without smoothing:', mnb_no.predict_proba(Xdocs))\n",
        "print('Probs with smoothing   :', mnb_yes.predict_proba(Xdocs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193ea00a",
      "metadata": {
        "id": "193ea00a"
      },
      "source": [
        "### P11. Train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e3ea20",
      "metadata": {
        "id": "50e3ea20"
      },
      "outputs": [],
      "source": [
        "# P11: GridSearchCV for SVM hyperparameters\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "Xw_train, Xw_test, yw_train, yw_test = train_test_split(Xw, yw, test_size=0.2, random_state=1, stratify=yw)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(probability=True, random_state=0))])\n",
        "param_grid = {'svc__C': [0.1, 1, 10], 'svc__gamma': ['scale', 0.1, 1], 'svc__kernel': ['rbf', 'linear']}\n",
        "gs = GridSearchCV(pipe, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "gs.fit(Xw_train, yw_train)\n",
        "print('Best params:', gs.best_params_)\n",
        "print('Best CV score:', gs.best_score_)\n",
        "print('Test accuracy with best estimator:', accuracy_score(yw_test, gs.best_estimator_.predict(Xw_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "260cda48",
      "metadata": {
        "id": "260cda48"
      },
      "source": [
        "### P12. Train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c50efb3",
      "metadata": {
        "id": "9c50efb3"
      },
      "outputs": [],
      "source": [
        "# P12: SVM on imbalanced dataset with class weighting\n",
        "from sklearn.datasets import make_classification\n",
        "Ximb, yimb = make_classification(n_samples=500, weights=[0.9,0.1], flip_y=0, random_state=0)\n",
        "Ximb_train, Ximb_test, yimb_train, yimb_test = train_test_split(Ximb, yimb, test_size=0.2, random_state=0)\n",
        "clf_unweighted = SVC(kernel='rbf', class_weight=None)\n",
        "clf_weighted = SVC(kernel='rbf', class_weight='balanced')\n",
        "clf_unweighted.fit(Ximb_train, yimb_train)\n",
        "clf_weighted.fit(Ximb_train, yimb_train)\n",
        "print('Unweighted recall (minority):', recall_score(yimb_test, clf_unweighted.predict(Ximb_test)))\n",
        "print('Weighted recall (minority):', recall_score(yimb_test, clf_weighted.predict(Ximb_test)))\n",
        "print('Unweighted accuracy:', accuracy_score(yimb_test, clf_unweighted.predict(Ximb_test)))\n",
        "print('Weighted accuracy:', accuracy_score(yimb_test, clf_weighted.predict(Ximb_test)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7965153e",
      "metadata": {
        "id": "7965153e"
      },
      "source": [
        "### P13. Implement a Naïve Bayes classifier for spam detection using email data (simple demo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "022c8bb4",
      "metadata": {
        "id": "022c8bb4"
      },
      "outputs": [],
      "source": [
        "# P13: Simple Naive Bayes spam detection demo (toy data)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "texts = ['win money now', 'cheap meds', 'hello how are you', 'meeting schedule', 'exclusive offer win']\n",
        "labels = [1,1,0,0,1]\n",
        "cv = CountVectorizer()\n",
        "Xtxt = cv.fit_transform(texts)\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xtxt, labels, test_size=0.4, random_state=0)\n",
        "mnb = MultinomialNB(alpha=1.0)\n",
        "mnb.fit(Xtr, ytr)\n",
        "print('Vocabulary:', cv.get_feature_names_out())\n",
        "print('Test predictions:', mnb.predict(Xte))\n",
        "print('Test probs:', mnb.predict_proba(Xte))\n",
        "print('Accuracy:', accuracy_score(yte, mnb.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfe88959",
      "metadata": {
        "id": "bfe88959"
      },
      "source": [
        "### P14. Train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9850997e",
      "metadata": {
        "id": "9850997e"
      },
      "outputs": [],
      "source": [
        "# P14: Compare SVM and Naive Bayes on the same dataset (Breast Cancer)\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "pipe_svm = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=0))])\n",
        "pipe_gnb = Pipeline([('scaler', StandardScaler()), ('gnb', GaussianNB())])\n",
        "pipe_svm.fit(Xtr, ytr)\n",
        "pipe_gnb.fit(Xtr, ytr)\n",
        "print('SVM accuracy:', accuracy_score(yte, pipe_svm.predict(Xte)))\n",
        "print('GaussianNB accuracy:', accuracy_score(yte, pipe_gnb.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55cbfa8",
      "metadata": {
        "id": "c55cbfa8"
      },
      "source": [
        "### P15. Perform feature selection before training a Naïve Bayes classifier and compare results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ef39e36",
      "metadata": {
        "id": "9ef39e36"
      },
      "outputs": [],
      "source": [
        "# P15: Feature selection (Univariate) before Naive Bayes\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "selector = SelectKBest(f_classif, k=10)\n",
        "X_sel = selector.fit_transform(Xc, yc)\n",
        "Xtr, Xte, ytr, yte = train_test_split(X_sel, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "clf = GaussianNB()\n",
        "clf.fit(Xtr, ytr)\n",
        "print('Accuracy after selecting top-10 features:', accuracy_score(yte, clf.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bad108a",
      "metadata": {
        "id": "5bad108a"
      },
      "source": [
        "### P16. Train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0e7c386",
      "metadata": {
        "id": "d0e7c386"
      },
      "outputs": [],
      "source": [
        "# P16: One-vs-Rest vs One-vs-One on Wine dataset\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xw, yw, test_size=0.2, random_state=1, stratify=yw)\n",
        "ovr = OneVsRestClassifier(SVC(kernel='linear', probability=True))\n",
        "ovo = OneVsOneClassifier(SVC(kernel='linear', probability=True))\n",
        "ovr.fit(Xtr, ytr)\n",
        "ovo.fit(Xtr, ytr)\n",
        "print('OvR accuracy:', accuracy_score(yte, ovr.predict(Xte)))\n",
        "print('OvO accuracy:', accuracy_score(yte, ovo.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d44a892",
      "metadata": {
        "id": "3d44a892"
      },
      "source": [
        "### P17. Train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fedf317",
      "metadata": {
        "id": "2fedf317"
      },
      "outputs": [],
      "source": [
        "# P17: Compare Linear, Poly, RBF SVM on Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "for kernel in ['linear', 'poly', 'rbf']:\n",
        "    pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel=kernel, probability=True, random_state=0))])\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    print(f'Kernel={kernel} | Test acc={accuracy_score(yte, pipe.predict(Xte)):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c381fc5",
      "metadata": {
        "id": "5c381fc5"
      },
      "source": [
        "### P18. Train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63d486f0",
      "metadata": {
        "id": "63d486f0"
      },
      "outputs": [],
      "source": [
        "# P18: Stratified K-Fold Cross-Validation average accuracy (SVM on Wine)\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=0))])\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "scores = cross_val_score(pipe, Xw, yw, cv=skf, scoring='accuracy')\n",
        "print('CV accuracies:', scores)\n",
        "print('Mean CV accuracy:', np.mean(scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6299aa5",
      "metadata": {
        "id": "b6299aa5"
      },
      "source": [
        "### P19. Train a Naïve Bayes classifier using different prior probabilities and compare performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e349065",
      "metadata": {
        "id": "0e349065"
      },
      "outputs": [],
      "source": [
        "# P19: Naive Bayes with different prior probabilities\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "# Default priors\n",
        "gnb_default = GaussianNB()\n",
        "gnb_default.fit(Xtr, ytr)\n",
        "# Custom priors (e.g., favor class 0)\n",
        "gnb_custom = GaussianNB(priors=[0.7, 0.3])\n",
        "gnb_custom.fit(Xtr, ytr)\n",
        "print('Default priors acc:', accuracy_score(yte, gnb_default.predict(Xte)))\n",
        "print('Custom priors acc :', accuracy_score(yte, gnb_custom.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "067b1892",
      "metadata": {
        "id": "067b1892"
      },
      "source": [
        "### P20. Perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c59812fb",
      "metadata": {
        "id": "c59812fb"
      },
      "outputs": [],
      "source": [
        "# P20: Recursive Feature Elimination (RFE) before SVM\n",
        "from sklearn.feature_selection import RFE\n",
        "wine = datasets.load_wine()\n",
        "Xw, yw = wine.data, wine.target\n",
        "estimator = SVC(kernel='linear')\n",
        "rfe = RFE(estimator, n_features_to_select=8)\n",
        "Xr = rfe.fit_transform(Xw, yw)\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xr, yw, test_size=0.2, random_state=1, stratify=yw)\n",
        "clf = SVC(kernel='rbf')\n",
        "clf.fit(Xtr, ytr)\n",
        "print('Accuracy after RFE (SVM):', accuracy_score(yte, clf.predict(Xte)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb4703bd",
      "metadata": {
        "id": "fb4703bd"
      },
      "source": [
        "### P21. Train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206f194b",
      "metadata": {
        "id": "206f194b"
      },
      "outputs": [],
      "source": [
        "# P21: Evaluate SVM using Precision, Recall, F1\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=0))])\n",
        "pipe.fit(Xtr, ytr)\n",
        "y_pred = pipe.predict(Xte)\n",
        "print('Precision:', precision_score(yte, y_pred))\n",
        "print('Recall   :', recall_score(yte, y_pred))\n",
        "print('F1-score :', f1_score(yte, y_pred))\n",
        "print('\\nClassification report:\\n', classification_report(yte, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e08c504",
      "metadata": {
        "id": "3e08c504"
      },
      "source": [
        "### P22. Train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b515815",
      "metadata": {
        "id": "4b515815"
      },
      "outputs": [],
      "source": [
        "# P22: Naive Bayes evaluated using Log Loss (Cross-Entropy)\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "clf = GaussianNB()\n",
        "clf.fit(Xtr, ytr)\n",
        "probs = clf.predict_proba(Xte)\n",
        "print('Log loss:', log_loss(yte, probs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad1f546",
      "metadata": {
        "id": "fad1f546"
      },
      "source": [
        "### P23. Train an SVM Classifier and visualize the Confusion Matrix using seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fac53a43",
      "metadata": {
        "id": "fac53a43"
      },
      "outputs": [],
      "source": [
        "# P23: Confusion matrix visualization using seaborn (SVM on Iris)\n",
        "iris = datasets.load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', random_state=0))])\n",
        "pipe.fit(Xtr, ytr)\n",
        "yp = pipe.predict(Xte)\n",
        "cm = confusion_matrix(yte, yp)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix (Iris)')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea9eaf48",
      "metadata": {
        "id": "ea9eaf48"
      },
      "source": [
        "### P24. Train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf87008",
      "metadata": {
        "id": "caf87008"
      },
      "outputs": [],
      "source": [
        "# P24: SVR evaluated with MAE instead of MSE\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "Xh, yh = housing.data, housing.target\n",
        "Xh_tr, Xh_te, yh_tr, yh_te = train_test_split(Xh, yh, test_size=0.2, random_state=0)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svr', SVR(kernel='rbf'))])\n",
        "pipe.fit(Xh_tr, yh_tr)\n",
        "yp = pipe.predict(Xh_te)\n",
        "print('SVR MAE:', mean_absolute_error(yh_te, yp))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ba4302",
      "metadata": {
        "id": "a7ba4302"
      },
      "source": [
        "### P25. Train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eafe2874",
      "metadata": {
        "id": "eafe2874"
      },
      "outputs": [],
      "source": [
        "# P25: Naive Bayes ROC-AUC (requires predict_proba)\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "clf = GaussianNB()\n",
        "clf.fit(Xtr, ytr)\n",
        "probs = clf.predict_proba(Xte)[:,1]\n",
        "print('ROC-AUC:', roc_auc_score(yte, probs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba6b3f2",
      "metadata": {
        "id": "0ba6b3f2"
      },
      "source": [
        "### P26. Train an SVM Classifier and visualize the Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867baae0",
      "metadata": {
        "id": "867baae0"
      },
      "outputs": [],
      "source": [
        "# P26: Precision-Recall Curve for SVM (Breast Cancer)\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "cancer = datasets.load_breast_cancer()\n",
        "Xc, yc = cancer.data, cancer.target\n",
        "Xtr, Xte, ytr, yte = train_test_split(Xc, yc, test_size=0.2, random_state=2, stratify=yc)\n",
        "pipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC(kernel='rbf', probability=True, random_state=0))])\n",
        "pipe.fit(Xtr, ytr)\n",
        "probs = pipe.predict_proba(Xte)[:,1]\n",
        "precision, recall, thresholds = precision_recall_curve(yte, probs)\n",
        "avg_prec = average_precision_score(yte, probs)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.plot(recall, precision)\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title(f'Precision-Recall curve (AP={avg_prec:.3f})')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}